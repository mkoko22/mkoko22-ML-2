# mkoko22-ML-2
IEEE-CIS Fraud Detection - Assignment 2

ამ პროექტის მიზანი იყო Kaggle-ის მიერ წარმოდგენილ მონაცემებზე თაღლითური ტრანზაქციების კლასიფიკაცია, მძიმე დისბალანსის მქონე ფინანსური ტრანზაქციების მონაცემთა ნაკრებიდან თაღლითური ოპერაციების (fraud) იდენტიფიცირება. მოცემულია მომხმარებლების ტრანზაქციების დეტალები და მიზანია isFraud ცვლადის პროგნოზირება.

გადავწყვიტე ერთობლივად მემუშავა train და test ფაილებზე, რომ მათი ერთგვაროვნება შემენარჩუნებინა. თავდაპირველად მოვახდინე მონაცემთა ტიპების ოპტიმიზაცია — სადაც შესაძლებელი იყო, int64-ები დავიყვანე int32/int16-ზე, ხოლო float64-ები — float32-ზე. ამით მნიშვნელოვნად შევამცირე მეხსიერების მოხმარება და kaggle-ის kernel 'out of memory' ერორი. 
ყველა მოქმედება ცალკე მეთოდში მოვაქციე. დავიწყე მონაცემების დამუშავებით. იქიდან გამომდინარე, რომ მონაცემები ძალიან მასშტაბური იყო, გადავწყვიტე თავიდან მომეშორებინა ცვლადები 80%-ზე მეტი NaN - მნიშვნელობებით. შემდეგ გადავამოწმე ვარიაცები, კონსტანტა მახასიათებლები და სხვადასხვა ტიპის ანომალიები. აღმოჩნდა, რომ მონაცემების დიდ ნაწილში ცალკეული თვისებების მნიშვნელობები ან ძალიან მაღალი იყო, ან ძალიან დაბალი — იმდენად, რომ ნორმალური განაწილების ფარგლებში თითქმის არ ექცეოდა. ამის გამო, 98%-დან 100%-მდე თვისება დაიდროპა, რადგან ისინი ანომალიური ბუნების გამო, ჩემი ვარაუდით, სასარგებლო ინფორმაციას არ შეიცავდნენ. შემდეგ გადავწყვიტე შემემოწმებინა ხომ არ კორელირებდნენ მახასიათებლები ერთმანეთთან, მაგრამ ჯერ კიდევ 300-მდე მახასიათებლით ხელში ყველას ერთიანად გადამოწმება არ იქნებოდა ოპტიმალური. მახასიათებლები ლოგიკურად ასე დავყავი - ბარათის მონაცემები, მისამართები, C - მნიშვნელობები და V - მნიშვნელობები. თავისმხრივ V - მნიშვნელობები 50 ცვლადისგან შემდგარ chunck - ებად დავყავი და ეტაპობრივად შევადარე ერთმანეთს. გამოვლინდა ასამდე კორელირებული მახასიათებელი, რომელთა კორელაციის მაჩვენებელი 0.95 - დან 1-მდე იყო. რადგან მახასიათებლების ნაკლებობას ისედაც არ ვუჩიოდი, კორელირებული მახასიათებლებიც მოვიშორე. Feature engineering- ის ეტაპზე შევქმენი ახალი თვისებები card1 ველზე დაყრდნობით. ჯგუფურად გავაერთიანე card1-ით განხორციელებული ტრანზაქციები და გამოვითვალე ამ ჯგუფების საშუალო, რაოდენობა, სტანდარტული გადახრა და ჯამური მნიშვნელობები. ეს თვისებები დავამატე როგორც train, ასევე test მონაცემებში, რათა მოდელს ჰქონოდა დამატებითი კონტექსტი ტრანზაქციის ანომალიურობის დასადგენად. მონაცემთა დამუშავება გავაგრძელე კატეგორიულ ცვლადებთან გამკლავებით, გამოვიყენე woe encoding. ხოლო feature importance- ის მეშვეობით ავარჩიე მოდელის გასაწვრთნელად ყველაზე მნიშვნელოვანი მახასიათებლები. დისბალანსის აღმოსაფხვრელად მინდოდა smote კლასის გამოყენება, მაგრამ kaggle - ზე არსებული გარემოსთან დაკავშირებული ერორი ვერაფრით გამოვასწორე და მონაცემთა დასაბალანსებლად მოვიძიე მეთოდი, რომელიც manual under sampling - ით აბალანსებდა მონაცემებს სასურველი თანაფარდობით. 

დავიწყე xgboost -მოდელზე ექსპერიმენტით. და ვცადე ჰიპერპარამეტრების ბევრი კომბინაცია, რამაც დამაბნია და გადავწყვიტე მათი კომბინაციები დამეგენერირებინა და ვალიდაციაზე საუკეთესო შედეგის კომბინაცია გამომეყენებინა. ( n_estimators=300,  max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8,  scale_pos_weight=1). საბოლოოდ Accuracy გამოვიდა 98%, Precision — 75.39% და f1 - 0.7, ვცადე გაუმჯობესება, მაგრამ ამაზე უკეთესი შედეგი მოდელმა ვერ დადო, ამიტომ გადავედი logistic regression და random forest - მოდელებზე. პირველმა სხვებთან შედარებით ყველაზე ცუდი შედეგი აჩვენა, ვცადე სხვადასხვა ოპტიმიზაცია, ვცადე Stochastic Gradient Descent, რომელმაც ძალიან დიდი დრო წაიღო და შედეგი ოდნავ გააუმჯობესა, ამიტომ რეგრესიაზე დიდი დრო აღარ დამიხარჯავს. ხოლო random forest - ის შემთხვევაში ვარეგულირე პარამეტრები და როცა მივხვდი, რომ ვეღარ ვაუმჯობესებდი, auc = 92% და f1 = 56% - ზე შევწყვიტე მოდელზე მუშაობა. ბოლოს გამოვიყენე adaboost, რომელმაც თავიდან ნორმალური შედეგი აჩვენა. DecisionTreeClassifier - ის და AdaBoostClassifier - ის გამოყენებით, უკეთ შერჩეული learning rate - ით (0.05) შედეგი საგრძნობლად გააუმჯობესა და f1 აწია 0.8-მდე ხოლო AUC 0.92. ზოგიერთ კომპონენტში აჯობა xgboost-ს. მაგრამ მაღალი AUC ის და სტაბილური precision-recall - ის გამო გადავწყვიტე დასაბმიტებისას გამომეყენებინა xgboost მოდელი. მოდელმა kaggle - ზე დასაბმიტებისას 0.86 შეფასება დააფიქსირა. 

მადლობა :)

